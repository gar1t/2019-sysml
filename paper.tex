\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{listings}

\lstset{
  aboveskip=12pt,
  basicstyle=\footnotesize\ttfamily,
  breaklines=true,
  captionpos=b,
  frame=tb,
  framexbottommargin=5pt,
  framextopmargin=5pt,
}

\usepackage{enumitem}

\setlist[description]{
  itemsep=4pt,
  labelindent=1em,
  leftmargin=1em,
  topsep=4pt,
}

\setlist[enumerate]{
  itemsep=4pt,
  topsep=4pt,
}

\setlist[itemize]{
  itemsep=4pt,
  topsep=4pt,
  labelindent=1em,
  leftmargin=1em,
}

% Use the following line for the initial blind version submitted for review:
\usepackage{sysml2019}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{sysml2019}

% The \sysmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\sysmltitlerunning{Guild AI: NEEDS A TITLE}

\begin{document}

\twocolumn[
\sysmltitle{Guild AI: Package management for machine learning models}

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

\begin{sysmlauthorlist}
\sysmlauthor{Garrett Smith}{guild}
\end{sysmlauthorlist}

\sysmlaffiliation{guild}{Guild AI, Chicago, Illinois, USA}

\sysmlcorrespondingauthor{Garrett Smith}{garrett@guild.ai}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\sysmlkeywords{Guild AI, SysML}

\vskip 0.3in

\begin{abstract}

  We present Guild AI, an open source toolkit that facilitates model
  reuse for application development by applying traditional software
  packaging constructs to the domain of machine learning. Package
  management strategies are central to successful software
  ecosystems. Examples include \href{https://www.npmjs.com/}{npm} for
  JavaScript, \href{https://wiki.debian.org/Apt}{APT} for Debian, and
  \href{https://pip.pypa.io}{pip} for Python. Such tools excel at
  creating traditional software packages. Machine learning
  applications however present unique requirements that call for tool
  specialization. Guild AI combines the proven effectiveness of
  package managers with novel features to enable machine learning
  model reuse for application development across a variety of use
  cases.

\end{abstract}
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \sysmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\sysmlEqualContribution} % otherwise use the standard text.

\section{Introduction}

Code reuse is a central tenet in software development and is supported
by myriad techniques, abstractions, and tools. Effective code reuse
allows developers to leverage prior work in a consistent, reliable way
that saves time and improves software quality. In this paper we
consider code reuse in terms of \emph{packaged software} in the
context of machine learning.

\subsection{Traditional Packaged Software}

Packaged software is software that is organized in a way that can be
easily installed and used by \emph{users}. A user is a consumer of
software, rather than a producer or developer. A user may be software
developer, in the case of consuming software libraries or frameworks,
or she may be an end-user who uses software as a program with a
graphical or command line interface. In both cases, packaged software
may be provided in units that the user can install and start using
quickly and reliably.

This is an example of installing packaged software:

{\footnotesize
\begin{verbatim}
 $ pip install tensorflow
\end{verbatim}}

By running this command, a user is using a \emph{package manager}---in
this case the program \verb|pip|---to install a package named
\verb|tensorflow|. With this innocuous command, the user implicitly expects
a number of things:

\begin{itemize}
\item The Python module \verb|tensorflow| should be installed.
\item Additional Python modules required by \verb|tensorflow| should
  also be installed.
\item Installed software should work as expected on the target system.
\item At a future date \verb|tensorflow| may be similarly uninstalled
  along with its no-longer-used required packages.
\item The act of installing and uninstalling \verb|tensorflow| should
  not break or otherwise destabilize other installed software on the
  target system.
\end{itemize}

Such package management facilities are essential to code reuse at the
system level. Package managers allow users to maintain complex
software systems with simple commands in a way that preserves their
integrity. They support rapid experimentation with new software as
users are free to install, use, and evaluate packages with the
confidence that they can safely remove that software later.

Package managers promote healthy software ecosystems by facilitating
software \emph{publishing}. Software authors use packaging tools to
create the installable software units---\emph{packages}---that are so
easily consumed by users. By publishing packaged software, developers
signify that their software will work as advertised if installed using
the applicable tools.

Software ecosystems that are enabled by package managers include
Python, JavaScript, Ruby, Go, and R. The GNU/Linux operating system
itself is an ecosystem of package management ecosystems including
packaging schemes from Debian, RedHat, Gentoo, and Arch
distributions. Package managers are established cornerstones of
software reuse across languages and operating systems.

\subsection{Code Reuse and Machine Learning Models}

A \emph{machine learning model} is challenging to define as models
have different contexts. In the most general context, models are
mathematical concepts or language that represents a system (ref). In
the context of machine learning, models may be represented by computer
source code, either explicitly as declared structures, or implicitly
by way of executable instructions that perform model related tasks. A
\emph{trained model} is a model that has learned some representation
of a system through an automated process of optimization.

For the purpose of our discussion, a machine learning model is a
software representation of a specific mathematical concept that can be
trained by a computer. Models may be in various states:
\emph{uninitialized}, \emph{initialized}, \emph{partially trained},
\emph{fully trained}, etc. Regardless of state, the distinguishing
feature of machine learning models for our purpose is that they
support runnable operations.

Models---and in particular \emph{trained models}---are similar to
traditional software in that they can be executed to perform
computations. In fact, the purpose of training a model is to create a
computer program. The difference between a trained model and a human
authored program is one of method: a trained model is learned by an
algorithm while a human authored program is written manually.

While models may be represented in part by traditional installed
software---e.g. an imperative TensorFlow program that defines a model
graph structure when executed---models require more to be construed as
reusable code: \emph{models must support operations on novel data}. If
they cannot be applied to novel data---either for training or for
inference---they cannot be considered reusable software.

The remainder of this paper will outline an approach to package
management that incorporates \emph{model operations} and \emph{run
  management} as an enabler of code reuse for machine learning models.

\subsection{Related Work}

\subsubsection{Model zoos}

A common approach to sharing and reusing machine learning models is a
\emph{model zoo}---an online repository of reusable models that
includes source code, examples, documentation, and in some cases
pretrained models. The Caffe Model Zoo (ref) was one of the first
model zoos and has inspired several others (ref).

Model zoos encourage code reuse by consolidating related
models---e.g. based on underlying library implementation or
application domain---and providing consistent documentation and
repeatable methods for model use.

Model zoos, however, do not typically provide tools for packaging,
distributing, and using models beyond those of publishing source code
and documentation (e.g. GitHub, web sites, etc.)

\subsubsection{TensorFlow Hub and Keras Applications}

\href{https://www.tensorflow.org/hub/}{TensorFlow Hub modules} and
\href{https://keras.io/applications/}{Keras Applications} are examples
of framework level facilities that provide a higher degree of model
reuse vis-\'a-vis model zoos. Both TensorFlow Hub modules and Keras
application support tools and patterns for packaging, distributing,
and using model source code and associated pretrained models. They
emphasize the use of \emph{transfer learning} for applying pretrained
models to data from different domains.

\subsubsection{Software Packages and Source Code Repositories}

Models are commonly reused through traditional software distribution
channels: \emph{packages} and \emph{source code repositories}. In both
cases, model source code is provided along with varying levels of
documentation and supporting tools. Unlike model zoos and framework
facilities, the level of code reuse available through these channels
is spotty and inconsistent.

\subsection{Differences}

Guild AI combines elements from model zoos, framework packages, and
traditional software channels to provide a state of the art facility
for packaging, distributing and using machine learning models.

As with model zoos, users can discover Guild AI models, including
documentation. Guild AI additionally supports installation and direct
use of models through operations.

Guild AI superficially resembles TensorFlow Hub in that users can
discover, install, and use models quickly and easily. Guild AI however
differs considerably in its implementation. Guild AI does not use a
programming interface but instead supports its features by way of
declared configuration and system level tools. Guild AI packaging
facilities can be applied to any machine learning model
implementation.

Guild AI also supports the process of \emph{package maintenance},
where a third party---a package maintainer---creates usable packages
without necessarily coordinating with upstream model developers. This
is discussed in Section \ref{sec:package-maintainer}.

As with traditional software packages and source code repositories,
Guild AI supports a federated approach to software distribution. Users
are free to use Guild AI to create and distribute reusable models in a
variety of ways. Refer to Section \ref{sec:use-cases} for coverage of
various use case.

\section{Concepts}

This paper relies on a number of core concepts, which are defined
below.

\subsection{Projects}

A \emph{project} is a directory that contains model related
code. Model \emph{developers} maintain projects for their
models. Model \emph{users} maintain projects for their applications,
which may use models for training, inference, etc.

Projects are central to model development and use. Guild AI supports
project based workflow as described in Section
\ref{sec:streamline-workflow}.

\subsection{Guild File}

A \emph{Guild file} is a plain text file containing Guild AI related
configuration. The file is named \verb|guild.yml| and is typically
located in the root directory of a Guild AI enabled project.

Listing~\ref{lst:guild-file} illustrates a simple Guild file.

\begin{figure}
\begin{lstlisting}[
    caption=Simple Guild file example,
    label={lst:guild-file}]
model: mnist
description:
  Logistic regression classifier
  of MNIST digits
operations:
  train:
    main: train_mnist
    flags:
      batch-size: 32
      train-steps: 1000
  evaluate:
    main: eval_mnist
\end{lstlisting}
\end{figure}

\subsection{Packages}

A Guild AI \emph{package} is a Python wheel distribution (ref) that
was generated using the \verb|guild package| command. Guild AI
packages are generated using \verb|setuptools|. They contain Guild AI
specific metadata that advertises models they contain.

\subsection{Models}

A Guild AI \emph{model} is a representation of a machine learning
model that is defined in a Guild file. Models define \emph{operations}
(Section \ref{sec:operations}) that perform model related tasks such
as training or evaluating as well as \emph{resources} that operations
may require (Section \ref{sec:resources}).

Models may be defined according to the underlying architecture or
intended use of the model.

\subsection{Operations}
\label{sec:operations}

An \emph{operation} is an action that can be performed on a
model. Operations are part of a model definition.

Guild AI does not prescribe any particular set of operations that must
be defined for a model. Model developers are free to name operations
as they see fit, though Guild AI does encourage the use of consistent
naming conventions.

Below are some common operation names.

\begin{description}
\item[prepare] prepare a dataset for training
\item[train] train a model from scratch
\item[transfer-learn] train a model using transfer learning
\item[finetune] finetune a trained model
\item[evaluate] evaluate/test a model
\item[classify] use a trained model for classification
\item[compress] compress a trained model (e.g. using quantization,
  etc.)
\item[export-and-freeze] export a frozen inference
  graph\footnote{TensorFlow specific}
\item[tflite] generate a TF Lite file from a frozen
  model\footnotemark[\value{footnote}]
\end{description}

This list illustrates that model operations can be used to perform
tasks beyond just training and evaluation. Feature rich models can
encompass all of the operations associated with a machine learning
development pipeline, from data acquisition and preparation through
training and optimization to application deployment and monitoring.

\subsection{Flags}

\emph{Flags} are parameters that can be set or modified by users when
they run an operation. Flags are used as inputs to the underlying code
that implements an operation.

Below is an example of using a flag to set the \emph{learning rate}
for a \emph{train} operation.

{\footnotesize
\begin{verbatim}
 $ guild run train learning-rate=0.0001
\end{verbatim}}

\subsection{Resources}
\label{sec:resources}

A Guild AI \emph{resource} represents a set of source files that can
be made available as inputs to a model operation. Resources play an
important role in Guild AI's ability to effective use
models. Resources can be used to resolve project files, download files
from URLs, unpack archives, and reference files generated by other
operation.

Listing~\ref{lst:resource} contains a resource definition for a
model. The resource \verb|idx-data| defines four source files that
will be made available to any operation that requires that resource.

\begin{figure}
\begin{lstlisting}[
    caption=Model resource example,
    label={lst:resource}]
model: mnist
resources:
  idx-data:
    description:
      MNIST dataset in compressed
      IDX format
    sources:
      - train-images-idx3-ubyte.gz
      - train-labels-idx1-ubyte.gz
      - t10k-images-idx3-ubyte.gz
      - t10k-labels-idx1-ubyte.gz
\end{lstlisting}
\end{figure}

\subsection{Runs}

A \emph{run} is a file system artifact---specifically a directory
created and managed by Guild AI---that contains metadata and files
generated by an operation. A run may also refer to the operation
system process associated with a running operation.

Runs are further described in Section \ref{sec:runs}.

\subsection{Run Directories}
\label{sec:run-dir}

A \emph{run directory} is a directory used for a run. When an
operation is run, the run directory is used as the operating system
process working directory. Operations should write run artifacts
within the working directory to ensure they are associated with the
run.

By default, Guild AI generates a new directory for each run and
creates links to required sources within that directory. Once the
directory is prepared, Guild AI starts the run operating system
process, which may read sources within the run directory.

\subsection{Environments}

Guild AI supports \emph{environments}, which are isolated runtime
environments used for installing and using models. Guild AI supports
environments created using
\href{https://virtualenv.pypa.io}{\emph{virtualenv}}.

\subsection{Workflows}

Guild AI \emph{workflows} are patterns of model use that are enabled
by running operations in specific order. Worflows are not currently
formal constructs in Guild AI but are used by convention and may be
implemented by other programs.

\section{Use Cases}
\label{sec:use-cases}

The package management facilities in Guild AI enable a number of use
cases, which are described below. In each case, the following roles
may be referenced:

\begin{description}
\item[user] person using a model developed by someone else
\item[developer] person developing a model, either from scratch or
  adapting the work of others
\item[packager] person neither using nor developing a model but who
  packages models created by developers for use by users
\end{description}

\subsection{Publish a Model for Use}
\label{sec:publish-model}

A \emph{developer} may encourage others to use her model by
documenting the steps needed to install, adapt, and use it. She may be
motivated to simplify the user experience so that her work is more
easily and successfully applied. The benefit to her may be quality
feedback from expert users, improved reputation, a sense of
contribution, or her colleagues may simply expect that work be
documented and easy to use.

While documenting the steps needed to use a model is undoubtedly
helpful, the process is still burdensome for both the developer and
her users. Guild AI can be used to simultaneously document model use
but also directly support its use through model operations.

Consider these instructions for model use, which might be documented
for a project:

\setlength{\parindent}{1em}
\begin{enumerate}
\item Install required packages
\item Download the raw data
\item Prepare the raw data for training
\item Train the model
\item Export the trained model for deployment
\end{enumerate}
\setlength{\parindent}{0em}

While these steps provide invaluable information, they still rely on
the user to carefully execute them and troubleshoot issues as they
arise. The process takes time and is subject to error at any step.

Guild AI can be used to both document and automate these
steps. Listing~\ref{lst:published-model} illustrates a Guild file that
might be used to codify the steps as model operations.

\begin{figure}
\begin{lstlisting}[
    caption=Sample project Guild file,
    label={lst:published-model}]
model: sample
operations:
  prepare-data:
    description:
      Prepare data for training
    main: prepare_data
    requires: raw-data
  train:
    description: Train model
    main: train
    requires: prepared-data
  export-model:
    description:
      Export model for deployment
    main: export_model
    requires: trained-model
resources:
  raw-data:
    description: Raw data for training
    sources:
      - url: http://mydata.com/raw.tgz
  prepare-data:
    description: Prepared data
    sources:
      - operation: prepare-data
  trained-model:
    description: Trained model
    sources:
      - operation: train
\end{lstlisting}
\end{figure}

The operations can be run using the Guild AI command line interface as
follows:

{\footnotesize
\begin{verbatim}
 $ guild run prepare-data
 $ guild run train
 $ guild run export-model
\end{verbatim}}

Furthermore, Guild AI provides model help, detailing available
operations, supported flags, and operation dependencies.

A user can get help for the model by running:

{\footnotesize
\begin{verbatim}
 $ guild help
\end{verbatim}}

Documentation is automatically generated from the model description,
making Guild AI models self documenting.

The developer can chose to distribute her model as a
project---e.g. making it available as a GitHub repository. She may
alternatively package the model and deploy as an installable Python
wheel distribution artifact.

\subsection{Use a Published Model}
\label{sec:use-model}

The flip side of the use case outline in
Section~\ref{sec:publish-model} is that of the \emph{user}. A user is
interesting in taking the work of a model developer and applying it,
either by adapting her model to a new application or by integrating
it as into another project.

Users commonly rely on thorough documentation to effectively use
models. If a project is under documented or otherwise hard to work
with, it may not be used. This is a lost opportunity for both user and
developer that hinders adoption and model improvement.

As described in Section~\ref{sec:publish-model}, a published model is
simple to use. From the standpoint of the user, the model can be used
either by cloning the project repository---e.g. as in the case of
GitHub published models---or install the model as a Python
package. Assuming the project was published as a package named
\verb|sample-project|, the user might run these commands to use the
model:

\setlength{\parindent}{1em}

{\footnotesize\emph{Install the package}}

{\footnotesize
\begin{verbatim}
 $ guild install sample-project
\end{verbatim}}

{\footnotesize\emph{View package help}}

{\footnotesize
\begin{verbatim}
 $ guild help sample-package
\end{verbatim}}

{\footnotesize\emph{Use the model}}

{\footnotesize
\begin{verbatim}
 $ guild run sample:prepare-data
 $ guild run sample:train
 $ guild run sample:export-model
\end{verbatim}}

\setlength{\parindent}{0em}

This illustrates the usefulness of packaging models for
reuse. Packaged models in this case consist of more than the
underlying model code---they include additional information that makes
them trivial to use relative to traditional forms of model reuse.

\subsection{Package Third Party Models}
\label{sec:package-maintainer}

Complex software ecosystems such as GNU/Linux have formalized the role
of a software \emph{packager} (or \emph{package maintainer}). The goal
of a packager is to improve the value of a software distribution by
creating high quality software packages that users find
useful. Packers are often independent of the upstream software
projects they package, serving as intermediaries between the software
developers and distribution users.

Guild AI supports this pattern by allowing packagers to incorporate
upstream model sources into Guild packages, patching them as needed to
provide the desired functionality to users.

Consider a case where a model developer is overloaded with research
tasks and doesn't have time to provide documentation for using her
models or publishing them for use as described in
Section~\ref{sec:publish-model}. An independent \emph{packager} may
step up to create a package instead.

A packager must assume that upstream sources are available \emph{as
  is} and he cannot expect developers to fix bugs or add new features
(though he may certainly ask and hope). Packagers therefore must be
prepared to \emph{patch} model code as needed to get it to work as
expected.

Guild AI provides various means of patching model source to support
packagers in this role.

\subsection{Team Collaboration}

The roles of \emph{developer}, \emph{user}, and \emph{packager} often
exist within a single organization or team. Guild AI supports team
collaboration by supporting each role as a separate concern.

Within a typical organization structure, the roles map as follows:

\setlength{\parindent}{1em}
developer $\rightarrow$ \emph{research scientist}

user $\rightarrow$ \emph{software engineer}

packager $\rightarrow$ \emph{research engineer}
\setlength{\parindent}{0em}

Of course this is a rough mapping and not all organizations share this
structure. Whatever the job titles and division of labor, Guild AI
supports the three facets of collaboration: model development, model
use, and intermediation between those two points in the interest of
freeing up developer time and resources to focus on pure model
development.

\subsection{Streamline Model Developer Workflow}
\label{sec:streamline-workflow}

In the case of a single developer who has no interest in publishing
models or otherwise supporting users, Guild AI model abstractions
provide value. By implementing models as units of operations, as is
the case with Guild AI model publishing, developers ensure their
models are easy to use from the early stages of development. This
streamlines the development process as developers can repeatedly run
and iterate through model operations using high level commands with
well defined, self documenting interfaces.

\subsection{Manage Run Artifacts}

The final use case discussed involves managing tracking run artifacts.
During both model development and use, it's useful to formally manage
the artifacts both generated and consumed by model operation. This is
similar to source code revision control, where developers can move
backward and forward in time to view their work. In the case of Guild
AI, the revisions are the results of data preparation, training
experiments, model optimizations---anything that might be generated by
an operation.

Section~\ref{sec:runs} provides more details about Guild AI run
management.

\section{Using Models}

Section~\ref{sec:use-model} describes the case of using published
models. This section provides more details on model usage in Guild AI.

\subsection{Packaged Models}

Packaged models are models that have been bundles with supporting code
and metadata into a Python wheel distribution. To generate a package,
a developer or packager must include a \verb|package| definition in
the project Guild file and then run the \verb|guild package| command.

Listing~\ref{lst:package} contains a sample \verb|package| definition.

\begin{figure}
\begin{lstlisting}[
    caption=Sample package definition,
    label={lst:package}]
package: sample-project
version: 1.0
description: Sample packaged project
url: https://my.co/packages/sample
author: John Doe
author-email: john@my.co
license: Apache 2.0
requires: [keras, Pillow, matlab]
\end{lstlisting}
\end{figure}

Given a Guild file with a package definition, developers can use
Guild AI to generate a Python wheel distribution, which in turn can be
used by users to install package models. Developers can also upload
packages to PyPI to make the package available for search and
installation.

Guild AI models are installed like any other Python package. Users may
use Guild AI to install the package:

{\footnotesize
\begin{verbatim}
 $ guild install sample-project
\end{verbatim}}

Alternatively, a user may use pip:

{\footnotesize
\begin{verbatim}
 $ pip install sample-project
\end{verbatim}}

Guild AI package requirements may be included in requirements files
(e.g. \verb|requirements.txt|) or as dependencies of other Python
package distributions.

Package models are convenient for using models and their related
functionality when a user doesn't need to modify the model source
code. To use models and also have the option of modifying their source
code, a user should use project models (Section
\ref{sec:project-models}).

\subsection{Project Models}
\label{sec:project-models}

Project models are defined in a project Guild file---i.e. a file named
\verb|guild.yml| that resides in the top level of a project
directory. Users may run operations on project models by running them
from within the project directory or by referring the project directory
in the operation specified for the \verb|run| command.

Project models are used by users or developers who expect to modify
project files.

Unlike packaged models, project models do not need to be installed
using a command---they are available from the project itself. A user
need only change to the project directory to run model operations.

\subsection{Listing Available Models and Operations}

A user may list available models using the \verb|guild models|
command. This command lists packaged models and, if the command is run
within a project, project models.

Each model has one or more operations, which can be listed using the
\verb|guild ops| command.

The ability to discover available models and operations is a key
feature enabling model use in Guild AI. Without formally defining
these, users are left to their own to infer model capabilities from
available documentation and source code.

\subsection{Getting Help for Models}

A user may use the \verb|guild help| command to get details about a
model, its operations, operation flags, and resource
dependencies. This is similar to the \emph{man} facility (ref) on many
systems.

A user may also get help for a particular operation by using the
\verb|--help-op| option with the \verb|guild run| command. This is
similar to the \verb|--help| option available for many programs.

Figure~\ref{fig:op-help} contains sample help for the \verb|train|
operation of the \verb|gpkg.mnist/cnn| package model.

\begin{figure}
\begin{lstlisting}
Usage: guild run [OPTIONS] cnn:train
[FLAG]...

Train the CNN

Use 'guild run --help' for a list of
options.

Dependencies:
  mnist-dataset  Yann Lecun's MNIST
                 dataset in compressed
                 IDX format

Flags:
  batch-size  Number of images to
              include in a training
              batch (100)
  epochs      Number of epochs to train
              (10)
\end{lstlisting}
\caption{Sample operation help}
\label{fig:op-help}
\end{figure}

It is important to note that model and operation help is automatically
generated from model descriptions. This facility lowers the cost of
model documentation for developers and gives users a consistent
interface for discovering model capabilities and instructions for using
them.

\subsection{Running Operations}

Once a model is available---either as an installed package model or
defined in a project---it can be used by running \emph{operations}. An
operation is an action that can be performed on a model. Operations
both \emph{define and implement} model capabilities. The act of using
a model is the process of running operation.

Operations may define an entire application development life cycle for
one model or a host of models that work together.

Section~\ref{sec:operations} lists common operation names.

The act of running an operation starts a \emph{run}, which is both an
operating process and a Guild AI managed file system artifact that
contains run details including generated output.

A user can run an operation using the \verb|guild run| command.

Refer to Section~\ref{sec:runs} for information about managing runs.

\subsection{Viewing and Comparing Runs}

As users and developers run model operations, they can view the
results in various ways. Guild AI provides a comprehensive command
line interface for managing runs, which is outlined in
Section~\ref{sec:runs}.

Guild AI also provides various methods for visualizing runs. Guild
View is an application that is provided with Guild AI that launches a
web application that can be used to view run details, including run
files.

Guild AI also integrates TensorBoard (ref) as a visualization tool,
which can be used to view TensorFlow event logs that are generated by
model operations.\footnote{Operations must explicitly write TensorFlow
  event logs during a run to support TensorBoard visualization.}

\subsection{Accessing Generated Artifacts}

Operations generate \emph{artifacts}, which are Guild AI managed files
stored on the local file system. Guild AI does not attempt to
obfuscate generated artifacts---they are maintained in the run
directory as written during the operation.

Artifacts are routinely used as inputs to subsequent operations. For
example, a TensorFlow checkpoint generated by a \verb|train| operation
may be used as input to an \verb|export-and-freeze| operation, which
in turn generates a frozen inference graph. This process is automatic
and part of Guild AI's resource resolution process.

Artifacts may also be accessed directly by users given their file
system paths. A user can list the full paths for all files associated
with a run using the \verb|guild runs info| command with the
\verb|-ff| option. With these path, the user may copy the files or
link to them as needed.

Runs may also be \emph{exported} to a location using the
\verb|guild export| command. Exported runs are copied (or moved, if
the \verb|--move| option is specified) to a specified file system
directory. This can be used to create backups or to transfer runs to
another location.

\subsection{Isolating Runs Using Environments}

The discussion of using models to this point has not mentioned
\emph{environments} as environment are optional for using Guild
AI. However, environments are useful for isolating installed packages
and runs on a system.

When running Guild AI operations in an activated environment, the
following are environment specific:

\begin{itemize}
\item Installed packages and therefore installed models
\item Runs
\end{itemize}

Environments are created using the \verb|guild init| command. Once
created, an environment must be explicitly activated using the
\verb|source guild-env| command.

Environments are useful isolating project level work. It's not uncommon
to have separate environments for each project.

\section{Managing Runs}
\label{sec:runs}

A \emph{run} is a formally managed file system artifact generated by
an operation. Runs may also refer to the operating system process
associated with a running operation.

Runs contain critical information for tracking and understanding what
happened during an operation:

\begin{itemize}
\item Run metadata such as the associated model and operation, status,
  start and stop times, operating system command and environment, and
  resolved dependencies
\item Output generated by the run---specifically the operating system
  process standard output and standard error
\item Files used by the operation (resolved resource sources)
\item Files generated by the operation
\end{itemize}

\subsection{Starting a Run}

Runs are started using the \verb|guild run| command and referencing a
model operation. Operations may be run with additional flag values
provided on the command line in the format \verb|NAME=VALUE|.

Here's an example of running an operation with flag values:

{\footnotesize
\begin{verbatim}
 $ guild run mnist:train \
     steps=10000 \
     batch-size=64 \
     learning-rate=0.0001
\end{verbatim}}

\subsection{Listing Runs}

Runs may be listed using the \verb|guild runs| command. Runs are
displayed with their unique ID, operation name, start time, status,
and user label.

Listed runs may be filtered by status, operation, and label.

\subsection{Getting Run Information}

Information about one or more runs may be displayed using the
\verb|guild runs info| command. By default, high level run information
such as the run ID, operation, start time, command, exit status, and
operating system pid are displayed. Additional command options may be
used to view more run information including files, flag values,
dependencies, and operation output.

\subsection{Deleting Runs}

One or more runs may be deleted using the \verb|guild runs rm|
command. Runs may be deleted using status, operation, or label.

Guild AI will restore deleted runs with \verb|guild runs restore| and
permanently delete runs using \verb|guild runs purge|.

\section{Developing Models}

As discussed above, published models---deployed from installed
packages or as projects---can be used by running model
\emph{operations}. Operations generate \emph{artifacts} that can be
used in subsequent operations or as final outputs in an application
development workflow.

In this section we discuss how such models are created using Guild AI.

\subsection{Developing a Model from Scratch}

When developing a model from scratch, a developer may ask two
questions:

\begin{itemize}
\item What is the defining characteristic of the model?
\item What can users do with the model?
\end{itemize}

The answers to these questions will inform the decision of what to
name the model and its operations. These values can be stubbed out in
a Guild file to provide a scaffold for implementation.

In many cases, this is simply a matter of selecting a model name and
creating a \verb|train| operation. Listing~\ref{lst:pet-stub}
illustrates a straightforward pet classifier.

\begin{figure}
\begin{lstlisting}[
    caption=Sample Guild file for a pet classifier,
    label={lst:pet-stub}]
model: pets
description:
  Pet classifier using a CNN
operations:
  train:
    description: Train the classifier
    main: train
    flags:
      images:
        description:
          Directory containing the pet
          images to train
        required: yes
      epochs:
        description:
          Number of epochs to train
        default: 100
\end{lstlisting}
\end{figure}

With even a simple stub, a developer can formalize the model and what
it does. This stage clarifies the model's user interface before the
functionality is implemented.

With a basic skeleton in place, the developer can implement model
operations. In the case of the pet classifier
(Listing~\ref{lst:pet-stub}) this means implementing training support
in a file named \verb|train.py| that resides in the project directory.

Operations can be developed and tested early on using Guild AI. Each
time \verb|guild run| is used to test an operation, Guild AI generates
a distinct, trackable run. Runs may be compared over time to help
answer questions or resolve issues.

Guild AI provides two additional mechanisms to help model developers
implement operations. Both effect the \emph{run directory}
(Section~\ref{sec:run-dir}) used by Guild AI.

The first is the use the \verb|--run-dir| option
to the \verb|guild run| command. This option may be used to specify an
alternative directory for the run. This is helpful when the developer
doesn't want to generate new runs for early stage operation
development.

The second mechanism is the use of the \verb|--stage| option to the
\verb|guild run| command. This option may be used to stage a run in an
alternative directory without running the operation. This is helpful
when the developer wants to study the run directory layout or to run
low level commands or scripts during the development process.

The developer can continue to evolve the model operations as needed to
satisfy user requirements. This may include new operations or
additional operation flags, which give users more flexibility when
running an operation. The developer may similarly add new models to
the Guild file.

\subsection{Adapting Existing Model Code}

In many cases, a developer will already have working model code she
wants to incorporate into a formalized model definition. As with the
role of \emph{packager} (Section~\ref{sec:package-maintainer}), a
developer may even be required to work with upstream sources she
cannot easily change. Guild AI provides a number of features that
support the adaptation of existing model code into usable models.

The operation \verb|main| attribute specifies a Python main module to
run. In many cases, a developer can simply reference an existing
Python script to incorporate it into an operation. In cases where a
script doesn't already exist---for example, the model code is in a
Jupyter Notebook and not readily usable---the developer should create
a new script that contains the applicable model code and use it for an
operation.

Operation \emph{flags} specified in the form \verb|NAME=VALUE| as
arguments to the \verb|guild run| command are passed to the Python
main module as command line arguments in the form
\verb|--NAME VALUE|. For example, assuming an operation \verb|train|
with a main module \verb|train|, the command
\verb|guild run train epochs=100| would be translated to the Python
command \verb|python -m train --epochs 100|. In this way developers
can parameterize scripts via operation flags.\footnote{It is standard
  practice to use Python's argparse module to support named command
  line options following the getopt convention (ref) in a Python
  script.}

If a flag name doesn't correspond to the option name expected
(e.g. the train script expected the option \verb|num_epochs|) the
developer can specify an alternative argument name for a flag using
the \verb|arg-name| attribute.

\subsection{Patching}

In cases where a developer cannot modify upstream sources or chooses
not to (e.g. to simplify ongoing maintenance) he may choose to patch
model sources for use in a model. Guild AI supports two methods of
patching.

The first method is to run a post processing script on required model
sources. This can be specified using a \verb|post-process| command for
a model source that applies patch files to upstream sources. Guild AI
will use the patched versions for related operations.

The second method is to wrap upstream scripts and monkey patch (ref)
the Python environment as needed to change behavior. An example of
this can be found in the \verb|gpkg.slim| package, which patches the
TensorFlow \verb|slim.losses.softmax_cross_entropy| module to add
support for balanced weight training. This technique successfully adds
a useful feature while avoiding the need to modify the upstream train
support in TF Slim.\footnote{While one could argue that it would be
  better to submit a pull request with the new functionality to
  upstream authors, there are always cases where this is not an
  option. Patching in these cases may be the only way to fix issues or
  add features.}

\subsection{Model Definition Reuse}

Guild AI provides a number of features that support model definition
reuse.

\subsubsection{Configuration Inheritance}

Guild files may contain \verb|config| section that contain arbitrary
configuration, which can be inherited by
models. Listing~\ref{lst:inheritance} defines two models, each
inheriting the properties of shared config. The shared config
\verb|model-base| defines the common model operations and uses
\emph{parameters} to define a model name and default learning rate.

\begin{figure}
\begin{lstlisting}[
    caption=Configuration inheritance,
    label={lst:inheritance}]
- config: model-base
  operations:
    train:
      main: train --model {{model-name}}
      flags:
        epochs: 100
        learning-rate: {{default-lr}}

- model: logreg
  extends: model-base
  params:
    model-name: logreg
    default-lr: 0.01

- model: cnn
  extends: model-base
  params:
    model-name: cnn
    default-lr: 0.0001
\end{lstlisting}
\end{figure}

Configuration inheritance can be applied across Guild files---e.g. to
configuration defined in packages or other projects. This is useful
for extending model definitions from other
developers. Listing~\ref{lst:extend-slim} demonstrates how a model can
inherit all of the operations of another model with a single line of
configuration.

\begin{figure}
\begin{lstlisting}[
    caption=Extending PNASNet Mobile,
    label={lst:extend-slim}]
- model: my-classifier
  extends: gpkg.slim.models/pnasnet-mobile
\end{lstlisting}
\end{figure}

Configuration inheritance has proven effective at managing complex
model configuration. This is evident in the \verb|gpkg.slim| and
\verb|gpkg.object-detection| packages, which make extensive use of
configuration inheritance to simplify their respective Guild files.

\subsubsection{Configuration Includes}

Configuration may also be reused by including shared configuration
files using \verb|include| objects. Listing~\ref{lst:shared} and
Listing~\ref{lst:include-shared} illustrate how includes can be used.

\begin{figure}
\begin{lstlisting}[
    caption=Shared configuration (e.g. defined in shared.yml),
    label={lst:shared}]
- config: model-base
  operations:
    train: train
\end{lstlisting}
\end{figure}

\begin{figure}
\begin{lstlisting}[
    caption=Use of includes (e.g. defined in guild.yml),
    label={lst:include-shared}]
- include: shared.yml

- model: model-a
  extends: model-base

- model: model-b
  extends: model-base
\end{lstlisting}
\end{figure}

Includes may also be used to merge shared configuration into other
configuration. Listing~\ref{lst:include-flags} illustrates how shared
configuration can be included into operation flags. The special
attribute \verb|$include| signifies that the corresponding value
defined in the referenced config should be merged into the current
value. The value for \verb|$include| can be a list to merge from
multiple sources. In the example, the value for \verb|learning-rate|
is redefined in the operation.

\begin{figure}
\begin{lstlisting}[
    caption=Including shared flag configuration,
    label={lst:include-flags}]
- config: default-train-flags
  flags:
    epochs: 100
    batch-size: 32
    learning-rate: 0.01

- model: sample
  operations:
    train:
      main: train
      flags:
        $include: default-train-flags
        learning-rate: 0.001
\end{lstlisting}
\end{figure}

\subsubsection{Reusing Python Main Modules}

The final method of model definition reuse described is that of
\emph{Python main module reuse}. Main modules---i.e. the modules
referenced in an operation \verb|main| specification---may reside in
any installed Python package. To reuse the main module, a developer
need only include the package reference in the main specification.

Listing~\ref{lst:tflite_convert} illustrates the use of the TensorFlow
module \verb|tflite_convert| to generate a TF Lite file for an
operation.

\begin{figure}
\begin{lstlisting}[
    caption=Reusing a Python main module,
    label={lst:tflite_convert}]
model: sample
operations:
  tflite:
    main: tensorflow/contrib/lite/python/tflite_convert
\end{lstlisting}
\end{figure}

Developers may create packages of reusable modules that can be easily
used to implement model operations. For example, consider an
organization that runs a specialized inference server that hosts
models deployed using an HTTP POST interface. The model development
team might create a package \verb|server-utils| that contains a Python
main module \verb|server_utils.deploy| that performs the
deployment.

Listing~\ref{lst:deploy} illustrates how such a facility could be
easily incorporated into a model to enable deployments. In the
example, the model is defined in a package that requires the
\verb|server-utils| package, which will be automatically installed by
the command \verb|guild install classifiers|.

\begin{figure}
\begin{lstlisting}[
    caption=Quickly enabling model deploy,
    label={lst:deploy}]
- package: classifiers
  requires: [server-utils]

- model: resnet-classifier
  operations:
    train: resnet_train
    deploy: server_utils.deploy
    requires: trained-model
  resources:
    trained-model:
      sources:
        - operation: train
\end{lstlisting}
\end{figure}

\subsection{Testing}

Users may trust that installed packages have undergone testing before
they being published. The goal of managing machine learning models as
packages, after all, is to ensure that models work as advertised. In
support of this goal, Guild AI lets developers and packagers specify
tests that can be used to model integrity.

Listing~\ref{lst:tests} contains two tests: one that verifies model
help---ensuring that what the user sees when she runs
\verb|guild help| is what the developer intends---and another that
exercises model operations and checks results.

\begin{figure}
\begin{lstlisting}[
    caption=Model tests,
    label={lst:tests}]
- test: help
  steps:
    - compare-help: test/help
- test: model
  steps:
    - run-op: prepare-samples
      flags:
        images: test/samples
    - run-op: train
      expected:
        - file: checkpoint
    - run-op: evaluate
      expected:
        - scalar: accuracy > 0.7
\end{lstlisting}
\end{figure}

The model test runs three operations: prepare-samples, train, and
evaluate. The train step asserts that a checkpoint file exist, which
confirms that the operation successfully saved a checkpoint. The
evaluate step asserts that the accuracy for the trained model is at
least 0.7.

Tests can be run using the \verb|guild test| command.

Tests may be run on an ad hoc basis or as a continuous integration
release process.

\subsection{Packaging}

Our final point of discussion on the topic of model development is
\emph{packaging}. Packaging consists of generating a Python wheel
distribution that contains the Guild file and all supporting project
code. The generated wheel distribution may be distributed internally
within an organization or published to PyPI.

Packages that are published to PyPI can be installed using the command
\verb|guild install| or using pip.

Packages are convenient for installing

\section{Implementation}

Guild AI is written in the Python programming language. It makes use
of a number of excellent open source libraries, many of which are
listed below.

\subsection{Command Line Interface}

Guild AI is primarily a command line tool. Users type Guild AI
commands in the form: \verb|guild COMMAND [ARG]...|

Guild AI makes use of the Click Python library (ref) to implement its
core command line processing support.

\subsection{Configuration Processing}

Guild AI supports YAML (ref) as its configuration language for both
Guild files and user configuration. It uses the PyYAML Python library
(ref) for low level parsing.

Guild AI has implemented a novel scheme for configuration reuse, which
supports multiple inheritance, file includes, and section
includes. The reuse support allows Guild AI developers to maintain
complex model configurations with easily managed abstractions.

\subsection{Resource Resolution}

Guild AI implements its own resource resolution scheme, which is used
to setup runs with required resource sources. Guild makes use of
directed acyclical graphs (DAGs) to model and efficiently resolve
complex interdependencies across operation.

\subsection{Model Testing}

Guild AI supports model testing through a novel configuration scheme
that integrates with model definitions. Tests may be defined for one
or more models, each running and verifying one or more operations.

\subsection{Packaging and Distribution}

Guild AI packages are valid Python wheel distributions. The underlying
libraries used to install Guild packages are the same as those used by
the \verb|pip| program to install PyPI packages. As a result, Guild
AI can be used to install PyPI packages and pip can be used to install
Guild AI packages---the process is the same in both cases.

\subsection{Run Indexing}

Guild AI uses a run indexing scheme to optimize run queries when
comparing runs and uses the Whoosh Python library (ref) in support
that.

\subsection{Run Visualization}

Guild AI provides various visualization tools, including its own run
viewer (Guild View) and integration with TensorBoard (ref).

Guild View is implemented using the VueJS library (ref) for the front
end and Werkzeug (ref) for the HTTP API back end.

\section{Status}

Guild AI is freely available under the Apache 2.0 open source
license. It has been under active development for the last two years
and undergone significant revision and updates during that time based
on user feedback and real world application development.

There have been several packaging initiatives that have produced
reusable models (ref) that fulfill the goals outlined in this
paper. Specifically, the packages support:

\begin{itemize}
\item Use without code changes
\item Reuse by developers who want to new models that leverage
  packaged functionality
\item Package maintainer workflow as described in
  Section~\ref{sec:package-maintainer}
\item Regression testing as a part of a continuous integration process
\end{itemize}

\bibliography{paper}
\bibliographystyle{sysml2019}

\end{document}
