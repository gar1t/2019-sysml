\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{listings}

\lstset{
  aboveskip=12pt,
  basicstyle=\footnotesize\ttfamily,
  breaklines=true,
  captionpos=b,
  frame=tb,
  framexbottommargin=5pt,
  framextopmargin=5pt,
}

\usepackage{enumitem}

\setlist[description]{
  itemsep=4pt,
  labelindent=1em,
  leftmargin=1em,
  topsep=4pt,
}

\setlist[enumerate]{
  itemsep=4pt,
  topsep=4pt,
}

\setlist[itemize]{
  itemsep=4pt,
  topsep=4pt,
}

% Use the following line for the initial blind version submitted for review:
\usepackage{sysml2019}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{sysml2019}

% The \sysmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\sysmltitlerunning{Guild AI: NEEDS A TITLE}

\begin{document}

\twocolumn[
\sysmltitle{Guild AI: Package management for machine learning models}

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

\begin{sysmlauthorlist}
\sysmlauthor{Garrett Smith}{guild}
\end{sysmlauthorlist}

\sysmlaffiliation{guild}{Guild AI, Chicago, Illinois, USA}

\sysmlcorrespondingauthor{Garrett Smith}{garrett@guild.ai}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\sysmlkeywords{Guild AI, SysML}

\vskip 0.3in

\begin{abstract}

  We present Guild AI, an open source toolkit that facilitates model
  reuse for application development by applying traditional software
  packaging constructs to the domain of machine learning. Package
  management strategies are central to successful software
  ecosystems. Examples include \href{https://www.npmjs.com/}{npm} for
  JavaScript, \href{https://wiki.debian.org/Apt}{APT} for Debian, and
  \href{https://pip.pypa.io}{pip} for Python. Such tools excel at
  creating traditional software packages. However, machine learning
  applications present unique requirements that call for tool
  specialization. Guild AI combines the proven effectiveness of
  package managers with novel features to enable machine learning
  model reuse for application development across a variety of use
  cases.

\end{abstract}
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \sysmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\sysmlEqualContribution} % otherwise use the standard text.

\section{Introduction}

Code reuse is a central tenet in software development and is supported
by myriad techniques, abstractions, and tools. Effective code reuse
allows developers to leverage prior work in a consistent, reliable
manner that saves time and improves software quality. In this paper we
consider code reuse in terms of \emph{packaged software} in the
context of machine learning.

\subsection{Traditional packaged software}

Packaged software is software that is organized in a way that can be
easily installed and used by \emph{users}. A user is a consumer of
software, rather than a producer or developer. A user may be software
developer, in the case of consuming software libraries or frameworks,
or she may be an end-user who uses software as a program using a
graphical or command line interface. In both cases, packaged software
may be provided in units that the user can install and start using
quickly and reliably.

This is an example of installing packaged software:

{\footnotesize
\begin{verbatim}
 $ pip install tensorflow
\end{verbatim}}

By running this command, a user is using a \emph{package manager}---in
this case the program \verb|pip|---to install a package named
\verb|tensorflow|. With this innocuous command, the user implicitly expects
a number of things:

\begin{itemize}
\item The Python module \verb|tensorflow| should be installed
\item Additional Python modules required by \verb|tensorflow| should
  also be installed
\item Installed software should work as expected on the target system
\item At a future date \verb|tensorflow| may be similarly uninstalled along
  with its no-longer-needed required packages
\item The act of installing and uninstalling \verb|tensorflow| should
  not break or otherwise destabilize other installed software on the
  target system
\end{itemize}

Such a package management facilities are essential to code reuse at
the system level. Package managers allow users to consume and manage
complex software systems with simple commands in a way that maintains
the integrity of other installed software. They support rapid
experimentation with new software as users are free to install, use,
and evaluate packages with the confidence that they can safely remove
that software later.

Package managers promote healthy software ecosystems by facilitating
software \emph{publishing}. Software authors use packaging tools to
create the installable software units---\emph{packages}---that are so
easily consumed by users. By publishing packaged software, developers
signify that their software will work as advertised if installed using
the supported tools.

Immense software ecosystems that are enabled by package managers
include Python, JavaScript, Ruby, Go, and R. The GNU/Linux operating
system itself is an ecosystem of package management ecosystems
including packaging schemes from Debian, RedHat, Gentoo, and Arch
distributions. Package managers are established cornerstones of
software reuse across languages and operating systems.

\subsection{Code reuse and machine learning models}

A \emph{machine learning model} is challenging to define as models
have different contexts. In the most general context, models are
mathematical concepts or language that represents a system (ref). In
the context of machine learning, models may be represented by computer
source code, either explicitly as declared structures, or implicitly
by way of executable instructions that perform model related tasks. A
\emph{trained model} is a model that has learned some representation
of a system through the process of computer based optimization.

For the purpose of our discussion, a machine learning model is a
software representation of a specific mathematical concept that can be
trained by a computer. Models may be in various states:
\emph{uninitialized}, \emph{initialized}, \emph{partially trained},
\emph{fully trained}. Regardless of state, the distinguishing feature
of machine learning models for our purpose is that they support
runnable computer operations.

Models---and in particular \emph{trained models}--- are similar to
traditional software in that they can be executed to perform
computations. In fact, the purpose of training a model is to create a
computer program. The difference between a trained model and a human
authored program is one of method: a trained model is learned by an
algorithm while a human authored program is written manually.

While models may be represented in part by traditional installable
software---e.g. a TensorFlow program that imperatively defines a model
graph structure when run---models require more to be construed as
reusable code: \emph{models must be trainable on novel data}.

The remainder of this paper will outline an approach to package that
incorporates \emph{model operations} and \emph{run management} as a
central enabler of code reuse for machine learning models.

\subsection{Related Work}

\subsubsection{Model zoos}

A common approach to sharing and reusing machine learning models is a
\emph{model zoo}---an online repository of reusable models that
includes source code, examples, documentation, and in some cases
pretrained models. The \href{Caffe Model Zoo}{Caffe Model Zoo} was one
of the first model zoos and has inspired several others (ref).

Model zoos encourage code reuse by consolidating related models
(e.g. based on underlying library implementation or application
domain) and providing consistent documentation and repeatable methods
for using the models.

Model zoos, however, do not typically provide tools for packaging,
distributing, and using models, beyond those of publishing source code
and documentation (e.g. GitHub, web sites, etc.)

\subsubsection{TensorFlow Hub and Keras Applications}

\href{https://www.tensorflow.org/hub/}{TensorFlow Hub modules} and
\href{https://keras.io/applications/}{Keras Applications} are examples
of framework level facilities that provide a higher degree of model
reuse vis-\'a-vis model zoos. Both TensorFlow Hub modules and Keras
application support tools and patterns for packaging, distributing,
and using model source code and associated pretrained models. They
emphasize the use of \emph{transfer learning} where applicable for
training models on novel datasets.

\subsubsection{Software packages and source code repositories}

Models are commonly reused through traditional software distribution
channels: packages and source code repositories. In both cases, model
source code is provided along with varying levels of documentation and
supporting tools. Unlike model zoos and framework packages, there is
little consistency in quality of code reuse available through these
channels. Developers are left to their own accord to find and use
available models.

\subsection{Differences}

Guild AI combines elements from model zoos, framework packages, and
traditional software channels to provide a state of the art facility
for packaging, distributing and using machine learning models.

As with model zoos, users can discover Guild AI models, including
documentation on their use. Guild AI additionally supports
installation and direct use of models through operations.

Guild AI superficially resembles TensorFlow Hub in that users can
discover, install, and use models quickly and easily. Guild AI however
differs considerably in its implementation. Guild AI does not present
a language level interface but instead defines a set of conventions
that are supported through configuration. Guild AI packaging
facilities can be applied to any machine learning model
implementation.

Guild AI also supports the well established practice of package
maintenance, where a third party can create usable packages without
necessarily coordinating with model developers. This is discussed in
detail in Section \ref{sec:package-maintainer}.

As with traditional software packages and source code repositories,
Guild AI supports a federated approach to software distribution. Users
are free to use Guild AI to create and distribute reusable models in a
variety of ways. Refer to Section \ref{sec:use-cases} for coverage of
various use case.

\section{Concepts}

This paper relies on a number of core concepts, which are defined
below.

\subsection{Projects}

A \emph{project} is a directory that contains model related
code. Model developers maintain projects for their models. Model users
maintain projects for the code that works with models, possibly
training them or otherwise incorporating them into an application.

Projects are central to model development and use. Guild AI supports
project based workflow as described in Section
\ref{sec:streamline-workflow}.

\subsection{Guild File}

A \emph{Guild file} is a plain text file containing Guild AI related
configuration. The file is named \verb|guild.yml| and is typically
located in the root directory of a Guild AI enabled project.

Listing~\ref{lst:guild-file} illustrates a simple Guild file.

\begin{lstlisting}[
    caption=Simple Guild file example,
    label={lst:guild-file}]
model: mnist
description:
  Logistic regression classifier
  of MNIST digits
operations:
  train:
    main: train_mnist
    flags:
      batch-size: 32
      train-steps: 1000
  evaluate:
    main: eval_mnist
\end{lstlisting}

\subsection{Packages}

A Guild AI \emph{package} is a Python wheel distribution (ref) that
was generated using the \verb|guild package| command. Guild AI
packages are no different from standard Python wheel distributions and
are generated using \verb|setuptools|. However, they contain Guild AI
specific entry points that effectively publish the models they
contain.

\subsection{Models}

A \emph{model} in Guild AI is a representation of a machine learning
model that is defined in a Guild file. Models define \emph{operations}
(Section \ref{sec:operations}) that perform model related tasks such
as training or evaluating as well as \emph{resources} (Section
\ref{sec:resources}) that operations may require.

Models may be defined according to the underlying architecture or
intended use of the model. Guild AI does not prescribe any formal
meaning to the term \emph{model} or what operations are supported.

\subsection{Operations}
\label{sec:operations}

An \emph{operation} is an action that can be performed on a
model. Operations are part of a model definition.

Guild AI does not prescribe any particular set of operations that must
be defined for a model. Model developers are free to name operations
as they see fit, though Guild AI does encourage naming conventions.

Below are some common operation names:

\begin{description}
\item[prepare] prepare a dataset for training
\item[train] train a model from scratch
\item[transfer-learn] train a model using transfer learning
\item[finetune] finetune a trained model
\item[evaluate] evaluate a model
\item[classify] use a trained model to classify inputs
\item[compress] compress a trained model (e.g. using quantization,
  etc.)
\item[export-and-freeze] export a frozen inference
  graph\footnote{TensorFlow specific}
\item[tflite] generate a TF Lite file from a frozen
  model\footnotemark[\value{footnote}]
\end{description}

This list illustrates that model operations can be used to perform a
host of tasks beyond training and evaluation. In fact, well designed
models can encompass all of the operations associated with a machine
learning application development pipeline, from data acquisition and
preparation to application deployment and monitoring.

\subsection{Flags}

\emph{Flags} are parameters that can be set or modified by users when
they run an operation. Flags are used as inputs to the underlying code
that implements an operation.

Below is an example of using a flag to set the \emph{learning rate}
for a \emph{train} operation.

{\footnotesize
\begin{verbatim}
 $ guild run train learning-rate=0.0001
\end{verbatim}}

\subsection{Resources}
\label{sec:resources}

A Guild AI \emph{resource} represents a set of source files that can
be made available as inputs to a model operation. Resources play a key
role in Guild AI's ability to effective use models. Resources can be
used to resolve project files, download files from URLs, unpack
archives, and reference files generated by other operation.

Listing~\ref{lst:resource} illustrates a resource definition for a
model. The resource \verb|idx-data| defines four source files that
will be made available to any operation that requires that resource.

\begin{lstlisting}[
    caption=Model resource example,
    label={lst:resource}]
model: mnist
resources:
  idx-data:
    description:
      MNIST dataset in compressed
      IDX format
    sources:
      - train-images-idx3-ubyte.gz
      - train-labels-idx1-ubyte.gz
      - t10k-images-idx3-ubyte.gz
      - t10k-labels-idx1-ubyte.gz
\end{lstlisting}

\subsection{Runs}

A \emph{run} is a file system artifact---specifically a directory
created and managed by Guild AI---that contains metadata and files
generated by an operation. A run may also refer to the operation
system process associated with a running operation.

Runs are described in detail in Section \ref{sec:runs}.

\subsection{Run Directories}
\label{sec:run-dir}

A \emph{run directory} is a directory used for a run. During a run
operation, the run directory is the system current working
directory. Operations should write run artifacts within the current
working directory to ensure that they are associated with the run.

By default, Guild AI generates a new directory for each run and
creates links to required resource sources within that directory. Once
the directory is prepared, Guild AI starts the run operating system
process, which then may access sources within the current working
directory.

Users may optionally specify an alternative run directory for a
run. This is commonly used by model developers during the process of
implementing model operation.s

\subsection{Environments}

Guild AI supports \emph{environments}, which are isolated runtime
environments used for installing and using models. Guild AI supports
environments created using
\href{https://virtualenv.pypa.io}{\emph{virtualenv}}.

\subsection{Workflows}

Guild AI \emph{workflows} are patterns of model use that are enabled
by running operations in specific order. Worflows are not currently
formal constructs in Guild AI but are referenced by convention or
documented usage.

\section{Use Cases}
\label{sec:use-cases}

The package management facilities in Guild AI enable a number of use
cases, which are described below. In each case, the following roles
may be referenced:

\begin{description}
\item[user] person using a model developed by someone else
\item[developer] person developing a model, either from scratch or
  adapting the work of others
\item[packager] person neither using nor developing a model but who
  packages models created by developers for use by users
\end{description}

\subsection{Publish a Model for Use}
\label{sec:publish-model}

A \emph{developer} may encourage others to use her model by
documenting the steps needed to install, adapt, and use it. She may be
motivated to simplify the user experience so that her work is more
easily and successfully applied. The benefit to her may be quality
feedback from expert users, improved reputation, a sense of
contribution, or her colleagues may simply expect that work be
documented and easy to use.

While documenting the steps needed to use a model is undoubtedly
helpful, the process is still burdensome for both the developer and
her users. Guild AI can be used to simultaneously document model use
but also directly support its use through model operations.

Consider these instructions for model use, which might be documented
for a project:

\setlength{\parindent}{1em}
\begin{enumerate}
\item Install required packages
\item Download the raw data
\item Prepare the raw data for training
\item Train the model
\item Export the trained model for deployment
\end{enumerate}
\setlength{\parindent}{0em}

While these steps provide invaluable information, they still rely on
the user to carefully execute them and troubleshoot issues as they
arise. The process takes time and is subject to error at any step.

Guild AI can be used to both document and automate these
steps. Listing~\ref{lst:published-model} illustrates a Guild file that
might be used to codify the steps as model operations.

\begin{lstlisting}[
    caption=Sample project Guild file,
    label={lst:published-model}]
model: sample
operations:
  prepare-data:
    description:
      Prepare data for training
    main: prepare_data
    requires: raw-data
  train:
    description: Train model
    main: train
    requires: prepared-data
  export-model:
    description:
      Export model for deployment
    main: export_model
    requires: trained-model
resources:
  raw-data:
    description: Raw data for training
    sources:
      - url: http://mydata.com/raw.tgz
  prepare-data:
    description: Prepared data
    sources:
      - operation: prepare-data
  trained-model:
    description: Trained model
    sources:
      - operation: train
\end{lstlisting}

The operations can be run using the Guild AI command line interface as
follows:

{\footnotesize
\begin{verbatim}
 $ guild run prepare-data
 $ guild run train
 $ guild run export-model
\end{verbatim}}

Furthermore, Guild AI provides model help, detailing available
operations, supported flags, and operation dependencies.

A user can get help for the model by running:

{\footnotesize
\begin{verbatim}
 $ guild help
\end{verbatim}}

Documentation is automatically generated from the model description,
making Guild AI models self documenting.

The developer can chose to distribute her model as a
project---e.g. making it available as a GitHub repository. She may
alternatively package the model and deploy as an installable Python
wheel distribution artifact.

\subsection{Use a Published Model}
\label{sec:use-model}

The flip side of the use case outline in
Section~\ref{sec:publish-model} is that of the \emph{user}. A user is
interesting in taking the work of a model developer and applying it,
either by adapting her model to a new application or by integrating
it as into another project.

Users commonly rely on thorough documentation to effectively use
models. If a project is under documented or otherwise hard to work
with, it may not be used. This is a lost opportunity for both user and
developer that hinders adoption and model improvement.

As described in Section~\ref{sec:publish-model}, a published model is
simple to use. From the standpoint of the user, the model can be used
either by cloning the project repository---e.g. as in the case of
GitHub published models---or install the model as a Python
package. Assuming the project was published as a package named
\verb|sample-project|, the user might run these commands to use the
model:

\setlength{\parindent}{1em}

{\footnotesize\emph{Install the package}}

{\footnotesize
\begin{verbatim}
 $ guild install sample-project
\end{verbatim}}

{\footnotesize\emph{View package help}}

{\footnotesize
\begin{verbatim}
 $ guild help sample-package
\end{verbatim}}

{\footnotesize\emph{Use the model}}

{\footnotesize
\begin{verbatim}
 $ guild run sample:prepare-data
 $ guild run sample:train
 $ guild run sample:export-model
\end{verbatim}}

\setlength{\parindent}{0em}

This illustrates the usefulness of packaging models for
reuse. Packaged models in this case consist of more than the
underlying model code---they include additional information that makes
them trivial to use relative to traditional forms of model reuse.

\subsection{Package Third Party Models}
\label{sec:package-maintainer}

Complex software ecosystems such as GNU/Linux have formalized the role
of a software \emph{packager} (or \emph{package maintainer}). The goal
of a packager is to improve the value of a software distribution by
creating high quality software packages that users find
useful. Packers are often independent of the upstream software
projects they package, serving as intermediaries between the software
developers and distribution users.

Guild AI supports this pattern by allowing packagers to incorporate
upstream model sources into Guild packages, patching them as needed to
provide the desired functionality to users.

Consider a case where a model developer is overloaded with research
tasks and doesn't have time to provide documentation for using her
models or publishing them for use as described in
Section~\ref{sec:publish-model}. An independent \emph{packager} may
step up to create a package instead.

A packager must assume that upstream sources are available \emph{as
  is} and he cannot expect developers to fix bugs or add new features
(though he may certainly ask and hope). Packagers therefore must be
prepared to \emph{patch} model code as needed to get it to work as
expected.

Guild AI provides various means of patching model source to support
packagers in this role.

\subsection{Team Collaboration}

The roles of \emph{developer}, \emph{user}, and \emph{packager} often
exist within a single organization or team. Guild AI supports team
collaboration by supporting each role as a separate concern.

Within a typical organization structure, the roles map as follows:

\setlength{\parindent}{1em}
developer $\rightarrow$ \emph{research scientist}

user $\rightarrow$ \emph{software engineer}

packager $\rightarrow$ \emph{research engineer}
\setlength{\parindent}{0em}

Of course this is a rough mapping and not all organizations share this
structure. Whatever the job titles and division of labor, Guild AI
supports the three facets of collaboration: model development, model
use, and intermediation between those two points in the interest of
freeing up developer time and resources to focus on pure model
development.

\subsection{Streamline Model Developer Workflow}
\label{sec:streamline-workflow}

In the case of a single developer who has no interest in publishing
models or otherwise supporting users, Guild AI model abstractions
provide value. By implementing models as units of operations, as is
the case with Guild AI model publishing, developers ensure their
models are easy to use from the early stages of development. This
streamlines the development process as developers can repeatedly run
and iterate through model operations using high level commands with
well defined, self documenting interfaces.

\subsection{Manage Run Artifacts}

The final use case discussed involves managing tracking run artifacts.
During both model development and use, it's useful to formally manage
the artifacts both generated and consumed by model operation. This is
similar to source code revision control, where developers can move
backward and forward in time to view their work. In the case of Guild
AI, the revisions are the results of data preparation, training
experiments, model optimizations---anything that might be generated by
an operation.

Section~\ref{sec:runs} provides more details about Guild AI run
management.

\section{Using Models}

Section~\ref{sec:use-model} describes the case of using published
models. This section provides more details on model usage in Guild AI.

\subsection{Packaged Models}

Packaged models are models that have been bundles with supporting code
and metadata into a Python wheel distribution. To generate a package,
a developer or packager must include a \verb|package| definition in
the project Guild file and then run the \verb|guild package| command.

Listing~\ref{lst:package} contains a sample \verb|package| definition.

\begin{lstlisting}[
    caption=Sample package definition,
    label={lst:package}]
package: sample-project
version: 1.0
description: Sample packaged project
url: https://my.co/packages/sample
author: John Doe
author-email: john@my.co
license: Apache 2.0
requires: [keras, Pillow, matlab]
\end{lstlisting}

Given a Guild file with a package definition, develolpers can use
Guild AI to generate a Python wheel distribution, which in turn can be
used by users to install package models. Developers can also upload
packages to PyPI to make the package available for search and
installation.

Guild AI models are installed like any other Python package. Users may
use Guild AI to install the package:

{\footnotesize
\begin{verbatim}
 $ guild install sample-project
\end{verbatim}}

Alternatively, a user may use pip:

{\footnotesize
\begin{verbatim}
 $ pip install sample-project
\end{verbatim}}

Guild AI package requirements may be included in requirements files
(e.g. \verb|requirements.txt|) or as dependencies of other Python
package distributions.

Package models are convenient for using models and their related
functionality when a user doesn't need to modify the model source
code. To use models and also have the option of modifying their source
code, a user should use project models (Section
\ref{sec:project-models}).

\subsection{Project Models}
\label{sec:project-models}

Project models are defined in a project Guild file---i.e. a file named
\verb|guild.yml| that resides in the top level of a projet
directory. Users may run operations on project models by running them
from within the project directory or by refering the project directory
in the operation specified for the \verb|run| command.

Project models are used by users or developers who expect to modify
project files.

Unlike packaged models, project models do not need to be installed
using a command---they are available from the project itself. A user
need only change to the project directory to run model operations.

\subsection{Listing Available Models and Operations}

A user may list available models using the \verb|guild models|
command. This command lists packaged models and, if the command is run
within a project, project models.

Each model has one or more operations, which can be listed using the
\verb|guild ops| command.

The ability to discover available models and operations is a key
feature enabling model use in Guild AI. Without formally defining
these, users are left to their own to infer model capabilities from
available documentation and source code.

\subsection{Getting Help for Models}

A user may use the \verb|guild help| command to get details about a
model, its operations, operation flags, and resource
dependencies. This is similar to the \emph{man} facility (ref) on many
systems.

A user may also get help for a particular operation by using the
\verb|--help-op| option with the \verb|guild run| command. This is
similar to the \verb|--help| option available for many programs.

Figure~\ref{fig:op-help} contains sample help for the \verb|train|
operation of the \verb|gpkg.mnist/cnn| package model.

\begin{figure}
  \begin{lstlisting}[]
  Usage: guild run [OPTIONS] cnn:train
  [FLAG]...

  Train the CNN

  Use 'guild run --help' for a list of
  options.

  Dependencies:
    mnist-dataset  Yann Lecun's MNIST
                   dataset in compressed
                   IDX format

  Flags:
    batch-size  Number of images to
                include in a training
                batch (100)
    epochs      Number of epochs to train
                (10)
  \end{lstlisting}
  \caption{Sample operation help}
  \label{fig:op-help}
\end{figure}

It is important to note that model and operation help is automatically
generated from model descriptions. This facility lowers the cost of
model documentation for developers and gives users a consistent
interface for discovering model capabilities and intructions for using
them.

\subsection{Running Operations}

Once a model is available---either as an installed package model or
defined in a project---it can be used by running \emph{operations}. An
operation is an action that can be performed on a model. Operations
both \emph{define and implement} model capabilities. The act of using
a model is the process of running operation.

Operations may define an entire application development life cycle for
one model or a host of models that work together.

Section~\ref{sec:operations} lists common operation names.

The act of running an operation starts a \emph{run}, which is both an
operating process and a Guild AI managed file system artifact that
contains run details including generated output.

A user can run an operation using the \verb|guild run| command.

Refer to Section~\ref{sec:runs} for information about managing runs.

\subsection{Viewing and Comparing Runs}

As users and developers run model operations, they can view the
results in various ways. Guild AI provides a comprehensive command
line interface for managing runs, which is outlined in
Section~\ref{sec:runs}.

Guild AI also provides various methods for visualizing runs. Guild
View is an application that is provided with Guild AI that launches a
web application that can be used to view run details, including run
files.

Guild AI also integrates TensorBoard (ref) as a visualization tool,
which can be used to view TensorFlow event logs that are generated by
model operations.\footnote{Operations must explicitly write TensorFlow
  event logs during a run to support TensorBoard visualization.}

\subsection{Accessing Generated Artifacts}

Operations generate \emph{artifacts}, which are Guild AI managed files
stored on the local file system. Guild AI does not attempt to
obfuscate generated artifacts---they are maintained in the run
directory as written during the operation.

Artifacts are routinely used as inputs to subsequent operations. For
example, a TensorFlow checkpoint generated by a \verb|train| operation
may be used as input to an \verb|export-and-freeze| operation, which
in turn generates a frozen inference graph. This process is automatic
and part of Guild AI's resource resolution process.

Artifacts may also be accessed directly by users given their file
system paths. A user can list the full paths for all files associated
with a run using the \verb|guild runs info| command with the
\verb|-ff| option. With these path, the user may copy the files or
link to them as needed.

Runs may also be \emph{exported} to a location using the
\verb|guild export| command. Exported runs are copied (or moved, if
the \verb|--move| option is specified) to a specified file system
directory. This can be used to create backups or to transfer runs to
another location.

\section{Managing Runs}
\label{sec:runs}

A \emph{run} is a formally managed file system artifact generated by
an operation. Runs may also refer to the operating system process
associated with a running operation.

Runs contain critical information for tracking and understanding what
happened during an operation:

\begin{itemize}
\item Run metadata such as the associated model and operation, status,
  start and stop times, operating system command and environment, and
  resolved dependencies
\item Output generated by the run---specifically the operating system
  process standard output and standard error
\item Files used by the operation (resolved resource sources)
\item Files generated by the operation
\end{itemize}

\subsection{Starting a Run}

Runs are started using the \verb|guild run| command and referencing a
model operation. Operations may be run with additional flag values
provided on the command line in the format \verb|NAME=VALUE|.

Here's an example of running an operation with flag values:

{\footnotesize
\begin{verbatim}
 $ guild run mnist:train \
     steps=10000 \
     batch-size=64 \
     learning-rate=0.0001
\end{verbatim}}

\subsection{Listing Runs}

Runs may be listed using the \verb|guild runs| command. Runs are
displayed with their unique ID, operation name, start time, status,
and user label.

Listed runs may be filtered by status, operation, and label.

\subsection{Getting Run Information}

Information about one or more runs may be displayed using the
\verb|guild runs info| command. By default, high level run information
such as the run ID, operation, start time, command, exit status, and
operating system pid are displayed. Additional command options may be
used to view more run information including files, flag values,
dependencies, and operation output.

\subsection{Deleting Runs}

One or more runs may be deleted using the \verb|guild runs rm|
command. Runs may be deleted using status, operation, or label.

Guild AI will restore deleted runs with \verb|guild runs restore| and
permanently delete runs using \verb|guild runs purge|.

\section{Developing Models}

As discussed above, published models---deployed from installed
packages or as projects---can be used by running model
\emph{operations}. Operations generate \emph{artifacts} that can be
used in subsquent operations or as final outputs in an application
development workflow.

In this section we discuss how such models are created using Guild AI.

\subsection{Developing a Model from Scratch}

When developing a model from scratch, a developer may ask two
questions:

\begin{itemize}
\item What is the defining characteristic of the model?
\item What can users do with the model?
\end{itemize}

The answers to these questions will inform the decision of what to
name the model and its operations. These values can be stubbed out in
a Guild file to provide a scaffold for implementation.

In many cases, this is simply a matter of selecting a model name and
creating a \verb|train| operation. Listing~\ref{lst:pet-stub}
illustrates a straightforward pet classifier.

\begin{lstlisting}[
    caption=Sample Guild file for a pet classifier,
    label={lst:pet-stub}]
model: pets
description:
  Pet classifier using a CNN
operations:
  train:
    description: Train the classifier
    main: train
    flags:
      images:
        description:
          Directory containing the pet
          images to train
        required: yes
      epochs:
        description:
          Number of epochs to train
        default: 100
\end{lstlisting}

With even a simple stub, a developer can formalize the model and what
it does. This stage clarifies the model's user interface before the
functionality is implemented.

With a basic skeleton in place, the developer can implement model
operations. In the case of the pet classifier
(Listing~\ref{lst:pet-stub}) this means implementing training support
in a file named \verb|train.py| that resides in the project directory.

Operations can be developed and tested early on using Guild AI. Each
time \verb|guild run| is used to test an operation, Guild AI generates
a distinct, trackable run. Runs may be compared over time to help
answer questions or resolve issues.

Guild AI provides two additional mechanisms to help model developers
implement operations. Both effect the \emph{run directory}
(Section~\ref{sec:run-dir}) used by Guild AI.

The first is the use the \verb|--run-dir| option
to the \verb|guild run| command. This option may be used to specify an
alternative directory for the run. This is helpful when the developer
doesn't want to generate new runs for early stage opertion
development.

The second mechanism is the use of the \verb|--stage| option to the
\verb|guild run| command. This option may be used to stage a run in an
alternative directory without running the operation. This is helpful
when the developer wants to study the run directory layout or to run
low level commands or scripts during the development process.

The developer can continue to evolve the model operations as needed to
satisfy user requirements. This may include new operations or
additional operation flags, which give users more flexibility when
running an operation. The developer may similarly add new models to
the Guild file.

\subsection{Adapting Existing Model Code}

In many cases, a developer will already have working model code she
wants to incorporate into a formalized model definition. As with the
role of \emph{packger} (Section~\ref{sec:package-maintainer}), a
developer may even be required to work with upstream sources she
cannot easily change. Guild AI provides a number of features that
support the adaptation of existing model code into usable models.

The operation \verb|main| attribute specifies a Python main module to
run. In many cases, a developer can simply reference an existing
Python script to incorporate it into an operation. In cases where a
script doesn't already exist---for example, the model code is in a
Jupyter Notebook and not readily usable---the developer should create
a new script that contains the applicable model code and use it for an
operation.

Operation \emph{flags} specified in the form \verb|NAME=VALUE| as
arguments to the \verb|guild run| command are passed to the Python
main module as command line arguments in the form
\verb|--NAME VALUE|. For example, assuming an operation \verb|train|
with a main module \verb|train|, the command
\verb|guild run train epochs=100| would be translated to the Python
command \verb|python -m train --epochs 100|. In this way developers
can parameterize scripts via operation flags.\footnote{It is standard
  practice to use Python's argparse module to support named command
  line options following the getopt convention (ref) in a Python
  script.}

If a flag name doesn't correspond to the option name expected
(e.g. the train script expected the option \verb|num_epochs|) the
developer can specify an alternative argument name for a flag using
the \verb|arg-name| attribute.

\subsection{Patching}

In cases where a developer cannot modify upstream sources or chooses
not to (e.g. to simplify ongoing mainteance) he may choose to patch
model sources for use in a model. Guild AI supports two methods of
patching.

The first method is to run a post processing script on required model
sources. This can be specified using a \verb|post-process| command for
a model source that applies patch files to upstream sources. Guild AI
will use the patched versions for related operations.

The second method is to wrap upstream scripts and monkey patch (ref)
the Python environment as needed to change behavior. An example of
this can be found in the \verb|gpkg.slim| package, which patches the
TensorFlow \verb|slim.losses.softmax_cross_entropy| module to add
support for balanced weight training. This technique successfully adds
a useful feature while avoiding the need to modify the upstream train
support in TF Slim.\footnote{While one could argue that it would be
  better to submit a pull request with the new functionality to
  upstream authors, there are always cases where this is not an
  option. Patching in these cases may be the only way to fix issues or
  add features.}

\subsection{Model Definition Reuse}

Guild AI provides a number of features that support model definition
reuse.

\subsubsection{Configuration Inheritance}

Guild files may contain \verb|config| section that contain arbitrary
configuration, which can be inherited by
models. Listing~\ref{lst:inheritance} defines two models, each
inheriting the properties of shared config. The shared config
\verb|model-base| defines the common model operations and uses
\emph{parameters} to define a model name and default learning rate.

\begin{lstlisting}[
    caption=Configuration inheritance,
    label={lst:inheritance}]
- config: model-base
  operations:
    train:
      main: train --model {{model-name}}
      flags:
        epochs: 100
        learning-rate: {{default-lr}}

- model: logreg
  extends: model-base
  params:
    model-name: logreg
    default-lr: 0.01

- model: cnn
  extends: model-base
  params:
    model-name: cnn
    default-lr: 0.0001
\end{lstlisting}

Configuation inheritance can be applied across Guild files---e.g. to
configuration defined in packages or other projects. This is useful
for extending model definitions from other
developers. Listing~\ref{lst:extend-slim} demonstrates how a model can
inherit all of the operations of another model with a single line of
configuration.

\begin{lstlisting}[
    caption=Extending PNASNet Mobile,
    label={lst:extend-slim}]
- model: my-classifier
  extends: gpkg.slim.models/pnasnet-mobile
\end{lstlisting}

Configuration inheritance has proven effective at managing complex
model configuration. This is evident in the \verb|gpkg.slim| and
\verb|gpkg.object-detection| packages, which make extensive use of
configuration inheritance to simplify their respective Guild files.

\subsubsection{Configuration Includes}

Configuration may also be reused by including shared configuration
files using \verb|include| objects. Listing~\ref{lst:shared} and
Listing~\ref{lst:include-shared} illustrate how includes can be used.

\begin{lstlisting}[
    caption=Shared configuration (e.g. defined in shared.yml),
    label={lst:shared}]
- config: model-base
  operations:
    train: train
\end{lstlisting}

\begin{lstlisting}[
    caption=Use of includes (e.g. defined in guild.yml),
    label={lst:include-shared}]
- include: shared.yml

- model: model-a
  extends: model-base

- model: model-b
  extends: model-base
\end{lstlisting}

Includes may also be used to merge shared configuration into other
configuration. Listing~\ref{lst:include-flags} illustrates how shared
configuration can be included into operation flags. The special
attribute \verb|$include| signifies that the corresponding value
defined in the referenced config should be merged into the current
value. The value for \verb|$include| can be a list to merge from
multiple sources. In the example, the value for \verb|learning-rate|
is redefined in the operation.

\begin{lstlisting}[
    caption=Including shared flag configuration,
    label={lst:include-flags}]
- config: default-train-flags
  flags:
    epochs: 100
    batch-size: 32
    learning-rate: 0.01

- model: sample
  operations:
    train:
      main: train
      flags:
        $include: default-train-flags
        learning-rate: 0.001
\end{lstlisting}

\subsubsection{Reusing Python Main Modules}

The final method of model definition reuse described is that of
\emph{Python main module reuse}. Main modules---i.e. the modules
referenced in an opertion \verb|main| specification---may reside in
any installed Python package. To reuse the main module, a developer
need only include the package reference in the main specification.

Listing~\ref{lst:tflite_convert} illustates the use of the TensorFlow
module \verb|tflite_convert| to generte a TF Lite file for an
operation.

\begin{lstlisting}[
    caption=Reusing a Python main module,
    label={lst:tflite_convert}]
model: sample
operations:
  tflite:
    main: tensorflow/contrib/lite/python/tflite_convert
\end{lstlisting}

Developers may create packages of reusable modules that can be easily
used to implement model operations. For example, consider an
organization that runs a specialized inference server that hosts
models deployed using an HTTP POST interface. The model development
team might create a package \verb|server-utils| that contains a Python
main module \verb|server_utils.deploy| that performs the
deployment.

Listing~\ref{lst:deploy} illustrates how such a facility could be
easily incorporated into a model to enable deployments. In the
example, the model is defined in a package that requires the
\verb|server-utils| package, which will be automatically installed by
the command \verb|guild install classifiers|.

\begin{lstlisting}[
    caption=Quickly enabling model deploy,
    label={lst:deploy}]
- package: classifiers
  requires: [server-utils]

- model: resnet-classifier
  operations:
    train: resnet_train
    deploy: server_utils.deploy
    requires: trained-model
  resources:
    trained-model:
      sources:
        - operation: train
\end{lstlisting}

\subsection{Testing models}

\subsection{Packaging}


\section{Implementation}

Guild AI is written in the Python programming language. It makes use
of a number of excellent open source libraries, many of which are
listed below.

\subsection{Command Line Interface}

Guild AI is primarily a command line tool. Users type Guild AI
commands in the form:

{\footnotesize
\begin{verbatim}
guild COMMAND [ARG]...
\end{verbatim}}

Guild AI makes use of the Click Python library (ref) to implement its
core command line processing support.

\subsection{Configuration Processing}

Guild AI supports YAML (ref) as its configuration language for both
Guild files and user configuration. It uses the PyYAML Python library
(ref) for low level parsing.

Guild AI has implemented a novel scheme for configuration reuse, which
supports multiple inheritance, file includes, and section
includes. The reuse support allows Guild AI developers to maintain
complex model configurations with easily managed abstractions.

\subsection{Resource Resolution}

Guild AI implements its own resource resolution scheme, which is used
to setup runs with required resource sources. Guild makes use of
directed acyclical graphs (DAGs) to model and efficiently resolve
complex interdependencies across operation.

\subsection{Model Testing}

Guild AI supports model testing through a novel configuration scheme
that integrates with model definitions. Tests may be defined for one
or more models, each running and verifying one or more operations.

\subsection{Packaging and Distribution}

Guild AI packages are valid Python wheel distributions. The underlying
libraries used to install Guild packages are the same as those used by
the \verb|pip| program to install PyPI packages. As a result, Guild
AI can be used to install PyPI packages and pip can be used to install
Guild AI packages---the process is the same in both cases.

\subsection{Run Indexing}

Guild AI uses a run indexing scheme to optimize run queries when
comparing runs and uses the Whoosh Python library (ref) in support
that.

\subsection{Run Visualization}

Guild AI provides various visualization tools, including its own run
viewer (Guild View) and integration with TensorBoard (ref).

Guild View is implemented using the VueJS library (ref) for the front
end and Werkzeug (ref) for the HTTP API back end.

\section{Status}

Guild AI is freely available under the Apache 2.0 open source
license. It has been under active development for the last two years
and undergone significant revision and updates during that time based
on user feedback and real world application development.

\subsection{History and Lessons Learned}

- Workshops
- User feedback

\subsection{Packages}

- slim, object-detection, mnist, hello

\subsection{Early Use}

\section{Summary}

\bibliography{paper}
\bibliographystyle{sysml2019}

\end{document}
